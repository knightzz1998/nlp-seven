{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1. jieba 中文分词处理"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Tokenizer.cut at 0x000001527DC81B48>\n",
      "\n",
      "全模式 : 大部/ 大部分/ 部分/ 情况/ 下/ ，/ 词汇/ 是/ 我们/ 对/ 句子/ 和文/ 文章/ 理解/ 的/ 基础/ ，/ 因此/ 需要/ 一个/ 工具/ 去/ 把/ 完整/ 的/ 文本/ 中分/ 分解/ 分解成/ 粒度/ 更/ 细/ 的/ 词/ 。\n",
      "\n",
      "精确模式 : 大部分/ 情况/ 下/ ，/ 词汇/ 是/ 我们/ 对/ 句子/ 和/ 文章/ 理解/ 的/ 基础/ ，/ 因此/ 需要/ 一个/ 工具/ 去/ 把/ 完整/ 的/ 文本/ 中/ 分解成/ 粒度/ 更细/ 的/ 词/ 。\n",
      "\n",
      "搜索引擎模式 : 大部/ 部分/ 大部分/ 情况/ 下/ ，/ 词汇/ 是/ 我们/ 对/ 句子/ 和/ 文章/ 理解/ 的/ 基础/ ，/ 因此/ 需要/ 一个/ 工具/ 去/ 把/ 完整/ 的/ 文本/ 中/ 分解/ 分解成/ 粒度/ 更细/ 的/ 词/ 。\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "text = \"大部分情况下，词汇是我们对句子和文章理解的基础，因此需要一个工具去把完整的文本中分解成粒度更细的词。\"\n",
    "\n",
    "cut_result = jieba.cut(text, cut_all=True) # 全模式\n",
    "print(cut_result)\n",
    "print(\"\\n全模式 : \" + \"/ \".join(cut_result))\n",
    "\n",
    "cut_result = jieba.cut(text, cut_all=False) # 精确模式\n",
    "print(\"\\n精确模式 : \" + \"/ \".join(cut_result))\n",
    "\n",
    "# 搜索引擎模式\n",
    "seg_result = jieba.cut_for_search(text)\n",
    "print( \"\\n搜索引擎模式 : \" + \"/ \".join(seg_result))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- jieba.lcut 以及 jieba.lcut_for_search 直接返回 list\n",
    "    1. 作用和 cut 与 cut_for_search 没有区别, 只是返回值不同"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 全模式 :  ['大家', '好', ',', ' ', '', '我', '很', '喜欢', '自然', '自然语言', '语言', '处理']\n",
      " 搜索模式 :  ['大家', '好', ',', ' ', '我', '很', '喜欢', '自然', '语言', '自然语言', '处理']\n"
     ]
    }
   ],
   "source": [
    "str = \"大家好, 我很喜欢自然语言处理\"\n",
    "lcut_result = jieba.lcut(str, cut_all=True) # 全模式\n",
    "print( \" 全模式 : \" , lcut_result)\n",
    "\n",
    "seg_lcut_result = jieba.lcut_for_search(str)\n",
    "print(\" 搜索模式 : \" , seg_lcut_result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.2 用户自定义词典\n",
    "\n",
    "- 作用 : 很多时候我们需要针对自己的场景进行分词，会有一些领域内的专有词汇。\n",
    "- 操作:\n",
    "    1. 可以用jieba.load_userdict(file_name)加载用户字典\n",
    "    2. 少量的词汇可以自己用下面方法手动添加：\n",
    "    3. 用 add_word(word, freq=None, tag=None) 和 del_word(word) 在程序中动态修改词典\n",
    "    4. 用 suggest_freq(segment, tune=True) 可调节单个词语的词频，使其能（或不能）被分出来。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(jieba.cut(\"如果放到旧字典中将出错\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}